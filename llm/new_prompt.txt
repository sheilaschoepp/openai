Your task is to create a new dense reward function in the context of reinforcement learning. 
I am currently using Mujoco to simulate a fetchreach robot to learn to do a task with the Proximal Policy Optimization RL algorithm. The aim is to make the robot learn to place it's end
effector at a particular position in a space. The robot is able to learn to do this using the following reward function:
    def compute_reward(self, achieved_goal, goal, info):
        # Compute distance between goal and the achieved goal.
        d = goal_distance(achieved_goal, goal)
        
        return -d
	where 
	    def _is_success(self, achieved_goal, desired_goal):
            d = goal_distance(achieved_goal, desired_goal)
            return (d < self.distance_threshold).astype(np.float32)
        
        def goal_distance(goal_a, goal_b):
            assert goal_a.shape == goal_b.shape
            return np.linalg.norm(goal_a - goal_b, axis=-1)

Now, we need a better reward function in the case the robot experiences a fault in it's mechanism. In this case, the fault is
named BrokenElbowFlexJoint. The fault is already mentioned in the robot.xml I provided you.

The info parameter of the reward function consists of a key 'joint_torques' representing the total torque of each joint mentioned in the xml file at each time step.
Each of these values is a sum of qfrc_constrained and qfrc_unc for each joint. 
Here is an example:
info['joint_torques']:
{'robot0:slide0': 76.4, 'robot0:torso_lift_joint': 49.1, 'robot0:head_pan_joint': 1.29e-08, 'robot0:head_tilt_joint': 2.65e-06, 'robot0:shoulder_pan_joint': -64.5, 'robot0:shoulder_lift_joint': -46.5, 'robot0:upperarm_roll_joint': -13.0, 'robot0:elbow_flex_joint': -17.7, 'robot0:forearm_roll_joint': -13.1, 'robot0:wrist_flex_joint': 2.24, 'robot0:wrist_roll_joint': -7.33, 'robot0:r_gripper_finger_joint': -2.0, 'robot0:l_gripper_finger_joint': 1.47}

info['joint_positions'] # a dict with keys as joint names and values as their float positions given by the sim.data.get_joint_qpos() method

info['joint_velocities'] # a dict with keys as joint names and values as their float velocities given by the sim.data.get_joint_qvel() method

Your task is to develop an optimal reward function using reward shaping techniques for the same task but in this fault environment, considering the torque values while minimizing energy usage
towards sub-optimal paths so that the robot learns more efficiently. Remember that your reward function will be used as is. Try to make such a reward function that gives out a smooth learning curve, learning very quickly and then plateauing well after convergence.
Define constants in the function itself if any.
Only give the reward function in your response incorporating everything inside it. Do not use markdown in your output and make the agent avoid things like reward hacking.
Be creative and create a detailed reward function. No need to import anything, you can use numpy as np.


